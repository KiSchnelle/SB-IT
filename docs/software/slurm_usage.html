<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Slurm usage</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Bricolage+Grotesque:wght@400;500;600;700&family=Spline+Sans:wght@300;400;500;600&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../styles/main.css">
</head>
<body>
  <div class="page">
    <div class="layout">
      <aside class="sidebar">
        <div class="brand">
          <div class="eyebrow">StrubiOS - EM Data Processing</div>
          <div class="brand-title">Guidelines</div>
        </div>
        <button class="nav-toggle" type="button" aria-expanded="false" aria-controls="primary-nav">Menu</button>
        <nav class="nav" id="primary-nav">
          <a href="../index.html">Overview</a>
          <div class="nav-group">
            <div class="nav-label">Cluster</div>
            <a href="../cluster_usage.html">Using the SB-HPC cluster</a>
            <a href="../cluster_resources.html">Cluster resources</a>
          </div>
          <div class="nav-group">
            <div class="nav-label">Sessions</div>
            <a href="../session_guidelines.html">Session and project guidelines</a>
            <a href="../particle_extraction.html">Extracting Particles</a>
            <a href="../data_cleanup.html">Data Cleanup</a>
            <a href="../data_transfer.html">Data Transfer</a>
          </div>
          <div class="nav-group">
            <div class="nav-label">Support</div>
            <a href="../troubleshooting.html">Troubleshooting</a>
          </div>
          <div class="nav-group">
            <div class="nav-label">Software</div>
            <a href="slurm_usage.html">Slurm usage</a>
            <a href="phenix.html">Using Phenix</a>
            <a href="topaz.html">Using Topaz</a>
            <a href="eman2.html">Using Eman2</a>
            <a href="cryolo.html">Using cryolo</a>
          </div>
        </nav>
      </aside>

      <main class="content">
        <header class="page-header reveal" style="--delay: 0.05s;">
          <h1>Slurm usage guidelines</h1>
        </header>

        <section class="section reveal" style="--delay: 0.1s;">
          <ul>
            <li>Use the <code>sbatch</code> command to submit jobs to the queue.</li>
            <li>Use the <code>squeue</code> command to check the status of your jobs.</li>
            <li>Use the <code>scancel</code> command to cancel jobs.</li>
            <li>Use the <code>sinfo</code> command to check the status of the cluster nodes.</li>
          </ul>
          <p>For interactive jobs, you can use the <code>srun</code> command. For example:</p>
          <pre><code>srun --partition=p.cryo --ntasks=1 --cpus-per-task=4 --mem=8G --pty bash</code></pre>
          <p>This command will allocate 1 task with 4 CPUs and 8 GB of memory on the <code>p.cryo</code> partition and open a bash shell.</p>
          <p>To submit a job script, you can create a file (e.g., <code>job.sh</code>) with the following content:</p>
          <pre><code>#!/bin/bash
#SBATCH --job-name=my_job
#SBATCH --output=my_job.out
#SBATCH --error=my_job.err
#SBATCH --partition=p.cryo
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=8G
# Load necessary modules
spack load relion
# Run your command
relion --some-option
</code></pre>
          <p>Then, you can submit the job script using:</p>
          <pre><code>sbatch job.sh</code></pre>
          <p>Often used additional options are:</p>
          <ul>
            <li><code>--time=HH:MM:SS</code> to set a time limit for the job.</li>
            <li><code>--gres=gpu:X</code> to request GPUs, where <code>X</code> is the number of GPUs needed.</li>
            <li><code>--nodes=X</code> to specify the number of nodes required.</li>
            <li><code>--exclusive</code> to request exclusive access to the allocated nodes.</li>
            <li><code>--tasks-per-node=X</code> to specify the number of tasks per node.</li>
          </ul>
          <p>For more information on how to use Slurm, you can refer to the <a href="https://slurm.schedmd.com/documentation.html">Slurm documentation</a>.</p>
        </section>

        <section class="section reveal" style="--delay: 0.2s;">
          <h2>Check queue and partitions</h2>
          <p>Use these quick commands to see node states and your jobs:</p>
          <pre><code>sinfo</code></pre>
          <pre><code>squeue -u $USER</code></pre>
          <p>To see detailed information about a job:</p>
          <pre><code>scontrol show job &lt;job_id&gt;</code></pre>
        </section>

        <section class="section reveal" style="--delay: 0.3s;">
          <h2>Choosing resources</h2>
          <p>Request only what you need to reduce queue time. Match <code>--cpus-per-task</code> to your software's thread count.</p>
          <p>Example CPU-only job with explicit time limit:</p>
          <pre><code>#!/bin/bash
#SBATCH --job-name=cpu_job
#SBATCH --partition=p.cryo
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=16G
#SBATCH --time=04:00:00
</code></pre>
        </section>

        <section class="section reveal" style="--delay: 0.4s;">
          <h2>Setting time limits</h2>
          <p>Always set a realistic time limit using <code>--time</code> in your script or command line.</p>
          <p>In a job script:</p>
          <pre><code>#SBATCH --time=HH:MM:SS</code></pre>
          <p>For an interactive session:</p>
          <pre><code>srun --partition=p.cryo --ntasks=1 --cpus-per-task=4 --mem=8G --time=02:00:00 --pty bash</code></pre>
        </section>

        <section class="section reveal" style="--delay: 0.5s;">
          <h2>GPU jobs</h2>
          <p>Request GPUs with <code>--gres=gpu:X</code> and match CPU and memory to the workload:</p>
          <pre><code>#!/bin/bash
#SBATCH --job-name=gpu_job
#SBATCH --partition=p.cryo
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=32G
#SBATCH --gres=gpu:1
#SBATCH --time=08:00:00
</code></pre>
        </section>

        <section class="section reveal" style="--delay: 0.6s;">
          <h2>Interactive sessions</h2>
          <p>Interactive jobs are ideal for testing, GUI tools, and short runs. Always do heavy work on a compute node, not the headnode:</p>
          <pre><code>srun --partition=p.cryo --ntasks=1 --cpus-per-task=4 --mem=8G --time=02:00:00 --pty bash</code></pre>
        </section>

        <section class="section reveal" style="--delay: 0.7s;">
          <h2>Job arrays</h2>
          <p>Use arrays for many similar jobs (e.g., processing multiple micrographs):</p>
          <pre><code>#!/bin/bash
#SBATCH --job-name=array_job
#SBATCH --partition=p.cryo
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=8G
#SBATCH --time=02:00:00
#SBATCH --array=1-100

echo \"Running task ${SLURM_ARRAY_TASK_ID}\"
</code></pre>
        </section>

        <section class="section reveal" style="--delay: 0.8s;">
          <h2>Output logs</h2>
          <p>Keep outputs organized with a dedicated logs folder. The <code>%x</code> token is the job name and <code>%j</code> is the job ID.</p>
          <pre><code>#SBATCH --output=logs/%x_%j.out
#SBATCH --error=logs/%x_%j.err</code></pre>
        </section>

        <section class="section reveal" style="--delay: 0.9s;">
          <h2>Monitoring and canceling</h2>
          <p>Common commands while a job is running:</p>
          <pre><code>squeue -u $USER</code></pre>
          <pre><code>scancel &lt;job_id&gt;</code></pre>
          <p>To cancel all your jobs:</p>
          <pre><code>scancel -u $USER</code></pre>
        </section>

        <section class="section reveal" style="--delay: 1.0s;">
          <h2>Hold and release jobs</h2>
          <p>Pause or resume a queued job without canceling it:</p>
          <pre><code>scontrol hold &lt;job_id&gt;</code></pre>
          <pre><code>scontrol release &lt;job_id&gt;</code></pre>
        </section>

        <section class="section reveal" style="--delay: 1.1s;">
          <h2>Exclusive node usage</h2>
          <p>Use exclusive mode when you need the full node to yourself:</p>
          <pre><code>#SBATCH --exclusive</code></pre>
          <p>You can also request a specific node if needed:</p>
          <pre><code>#SBATCH --nodelist=bert101</code></pre>
        </section>

        <section class="section reveal" style="--delay: 1.2s;">
          <h2>Post-run resource usage</h2>
          <p>Check actual CPU, memory, and runtime usage after a job finishes:</p>
          <pre><code>sacct -j &lt;job_id&gt; --format=JobID,State,Elapsed,MaxRSS,ReqMem,AllocCPUS</code></pre>
        </section>

        <section class="section reveal" style="--delay: 1.3s;">
          <h2>Good practices</h2>
          <ul>
            <li>Set a realistic <code>--time</code> to avoid premature termination.</li>
            <li>Use <code>--mem</code> based on actual usage, not peak estimates.</li>
            <li>Keep outputs and logs organized with <code>--output</code> and <code>--error</code>.</li>
          </ul>
        </section>
      </main>
    </div>
  </div>
  <script src="../scripts/site.js"></script>
</body>
</html>
